---
title: "HW5 - Problem 3 - Orange Juice classification"
author: "misken"
date: "March 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 3 - Predicting orange juice purchases

The dataset is available as part of the ISLR package. You can see the
documentation for that package or the following link describes the OJ
dataset - https://rdrr.io/cran/ISLR/man/OJ.html.

**SUGGESTION**: See the material available in Downloads_StatModels2 from the
session on classification models in R. In particular, the folder on
logistic regression and the example in the folder intro_class_HR/ will
be useful.

## Data prep

We'll do a little data prep to set things up so that we are trying to
predict whether or not the customer purchased Minute Maid (vs Citrus Hill.)
Just run the following chunks to load the dataset, do some data prep and
then partition the data into training and test sets.

```{r loaddata}
library(ISLR)
ojsales <- (ISLR::OJ)
```

Clean up the storeid related fields. Drop Store7 field.

```{r factors}
ojsales$StoreID <- as.factor(ojsales$StoreID)

# Create a new variable to act as the response variable.
ojsales$MM <- as.factor(ifelse(ojsales$Purchase=="MM",1,0))
```

Now we'll just take a subset of the columns as there are a few that contain the
same information. Remember, the new column `MM` is the one we are trying to
predict.

```{r subset}
ojsales_subset <- ojsales[, c(19, 3:13, 15:17)]
```

Just run this chunk to create training and test datasets. This way we'll
all be working with the same datasets. Notice that the test set is 10% of
the full dataset.

```{r partition}
set.seed(167)
sample_size <- ceiling(0.10 * nrow(ojsales))
testrecs <- sample(nrow(ojsales_subset),sample_size)
ojsales_test <- ojsales_subset[testrecs,]
ojsales_train <- ojsales_subset[-testrecs,]  # Negative in front of vector means "not in"
rm(ojsales_subset, ojsales) # No sense keeping a copy of the entire dataset around
```

## Your job

You should build at least two classification models to try to predict MM.
Our error metric will be overall accuracy.

Obviously, `ojsales_train` is your training dataset. After fitting each
model, use the `caret::confusionMatrix` function to create a confusion matrix
for each of the models based on the training data.

You should at least try the following two techniques:
- logistic regression
- a simple decision tree


```{r}
library(caret)
library(caretEnsemble)
library(ggplot2)
library(e1071)
library(randomForest)
library(VIF)
library(rpart)
library(rpart.plot)
library(dplyr)
library(ISLR)
```


```{r}
str(ojsales_train)
```

```{r}
summary(ojsales_train)
```
# Logrithmic Model

```{r}
Logmodel1 <- glm(formula = MM ~ StoreID + PriceMM + PriceDiff + StoreID*PriceMM + LoyalCH, family = binomial, data = ojsales_train)
```


```{r}
summary(Logmodel1)
```


```{r}
Prob_log <- predict(Logmodel1, newdata=ojsales_train, type="response")
yhat_dLogM <- (Logmodel1$fit > 0.5) * 1
Yhat_fact <- as.factor(yhat_dLogM)
cmlog <- caret::confusionMatrix(Yhat_fact, ojsales_train$MM, positive = "1")
cmlog
```


This was really bizare, I was going through the logistic Regression Food Stmps notes like you suggested and whenever I tried to load my predicted values into the confusion matrix it would say that the MM values and the Yhat values were on different levels. When I used the "Levels" function on the yhat values I got a null value returned. So forcing the Yhat values into an as.factor function fixed it. 




```{r}
Prob_testlog <- predict(Logmodel1, newdata=ojsales_test, type="response")
yhat_dLogMtest <- (Prob_testlog > 0.5) *1
Yhat_facttest <- as.factor(yhat_dLogMtest)
cmlogtest <- caret::confusionMatrix(Yhat_facttest, ojsales_test$MM, positive = "1")
cmlogtest
```
# Simple Decision Tree

```{r}
tree <- rpart(MM ~ StoreID + PriceMM + PriceDiff + LoyalCH, data=ojsales_train, method="class")
tree
rpart.plot(tree)
```

```{r}
head(predict(tree))

head(predict(tree, type="class"))
```

```{r}
cmtree <- caret::confusionMatrix(predict(tree, type="class"), 
                       ojsales_train$MM, positive = "1")
cmtree
```

```{r}
tree.pred <- predict(tree, ojsales_test, type = "class")
```

```{r}
cmtree2 <- caret::confusionMatrix(tree.pred, ojsales_test$MM, positive = "1")
cmtree2
```

# Bootstrap Aggregation


```{r}
oj.bag <- randomForest(MM ~ StoreID + PriceCH + PriceMM + DiscCH + DiscMM + SpecialCH + SpecialMM + LoyalCH + SalePriceMM + SalePriceCH + PriceDiff + PctDiscMM + PctDiscCH + ListPriceDiff, data = ojsales_train, mtry=14, importance=TRUE, na.action = na.omit)

oj.bag
```

```{r}
oj_imp <- arrange(as.data.frame(oj.bag$importance),MeanDecreaseGini)
oj_imp$variable <- as.factor(names(oj.bag$importance[,1]))
oj_imp <- within(oj_imp, variable <- reorder(variable, MeanDecreaseGini))
ggplot(data = oj_imp) + geom_bar(aes(x=variable, y=MeanDecreaseGini), stat = "identity") + coord_flip()
```



```{r}
oj.bag.pred <- predict(oj.bag, ojsales_test, type="class" )
oj.bag.pred <- caret::confusionMatrix(oj.bag.pred, ojsales_test$MM, positive = "1")
oj.bag.pred
```
# Random Forest

```{r}
oj.rf <- randomForest(MM ~ StoreID + PriceCH + PriceMM + DiscCH + DiscMM + SpecialCH + SpecialMM + LoyalCH + SalePriceMM + SalePriceCH + PriceDiff + PctDiscMM + PctDiscCH + ListPriceDiff, data = ojsales_train, mtry = 4, importance = TRUE, na.action = na.omit)
```

```{r}
oj.rf
```

```{r}
oj.rf.pred <- predict(oj.rf, ojsales_test, type = "class")
cm.rf.pred <- caret::confusionMatrix(oj.rf.pred, ojsales_test$MM, positive = "1")
cm.rf.pred
```

# Summary

The obvious winner in terms of accuracy here was the logistic model which had a test accuracy score of 90%. That could be caused by a small sample size especially in the test dataset, which in turn causes variance in the preformance metric. I checked the signs of the coeficients and all of them had the expected sign in front of them so multicolinearity was not the culprit. There could be other cuases as well, the point being that the 90% shouldn't be trusted as a reliable score on that model it is more likely that the structure of the data in the test data set played nice with the particular logistic model I made. 

The Bootstrap Aggregation model did the worst out of the four models tried. The simple decision tree and the random forest model tied for 2nd place. The reason the simple decision tree did so well is probably because I was able to decide what variables the tree used while in the random forest the algoritim "decided" which variables were important. 

While I don't completly trust the 90% of the logistic model, statistical methods should not be discounted just becuase there are new black box algorithims competing with them. In many cases a regression model with interaction terms and transformations can beat out a machine learning algorithim. Or an ensemble of regression models can be used to predict a numerical response variable where machine learning would fail. 

## Sensitivity
Whats interesting here is that even though the simple decision tree had a high predicition score on the confusion matrix, it had the lowest sensitivity score. The Bootstrap Aggregation method came in third while the random forest and logrithmic methods tied for first with a score of .77.

## Accuracy: Training vs. Test
All of the models had a better test score than trainning score, so the log model is not alone in being able to predict the test dataset better, though it had the most dramatic difference between test and train. This could be cuased by the structual makeup of the test data, the size of the test data, or the size of the overall dataset. 

## Overfitting
There was not any evidence of overfitting, in fact, we would have a hard time locating it as all of our test results were better than our train results. Our train results were not overly good, so I can't definitively say we have overfitting.

**HACKER EXTRA:** Try additional techniques such as random forest, k-nearest 
neighbor or others.

Then use the `predict()` function to make classification predictions on the
test dataset and use `caret::confusionMatrix` to create a confusion matrix
for each of the models for the predictions. 

Summarize your results. 
- Which technique performed the best in terms of overall accuracy? 
- Which technique had the best sensitivity score?
- How did accuracy differ for the training and test datasets for each model?
- Is their any evidence of overfitting?